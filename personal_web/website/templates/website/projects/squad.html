{% extends "website/base.html" %}

{% load static %}
{% block squad %}
<main>
    <br>
    <div class="container col-md-9">
        <h1>Fine-tune RoBERTa for Question-Answering with HyperColumn</h1>
        <br>
        <p>During the quarantine, I locked myself at a hotel in Shanghai for around 28 days. I followed the CS224N course from Stanford University and finished the final project. Inspired by the earlier computer vision project, I found that the RoBERTa-base model can be further improved by implementing the HyperColomn feature. Without much changing of the weights in attention layers, the early prediction could somehow increase the model accuracy in terms of the F1 score by around 1%.</p>
        <p>Hypercolumn originally stands for a group of nerve cells in the brain which help us identify objects instantly. In computer vision, people often use multiple convolutional layers to form a pyramid of the receptive region, with each layer being in a different resolution. For image segmentation tasks, the ultimate goal is to classify every pixel from learned features. We can use a vector of pixels on top of the original image as a representation of the corresponding pixel. When doing image segmentation, the top values in the vector gave us coarse information about what segment this pixel belongs to, the bottom values gave us precise information. This technology has been proven effective in image segmentation tasks.</p>
        <img src="{% static 'images/hypercolumn.jpg' %}" class="col-md-9"></img>
        <p>The question-answering task, however, is also a classification task, in that, we tried to classify the context into three groups, start position, end position, and other words. In the RoBERTa-base model, those stacks of attention layers and be viewed as the pyramid. When information was propagated from the bottom layer to the top layer, each layer could capture the information in different resolution levels. The original RoBERTa-base model only takes into consideration the output of the last attention layer as the input for the classification layer. As an analogy, the traditional RoBERTa-base model used the information from the bottom layer of the pyramid. When hypercolumn is implemented in RoBERTa-base, the F1 score increased from 83.42% to 84.27% on average of 10 consecutive training.</p>
        <p>The biggest challenge in Squad 2.0 data is that it has questions for which we cannot find the exact answers directly from the passages. The transformer-based models handle this problem by checking if the start position is actually before the end position, which is trivially correct. We can improve this by building an additional model on top of the original one aiming at classifying if the question is answerable or not.</p>
    </div>
</main>
{% endblock squad %}
